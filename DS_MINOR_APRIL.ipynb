{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "DS-MINOR-APRIL.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyarth10/Mini-Project/blob/main/DS_MINOR_APRIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHN2QZ0MGQWc"
      },
      "source": [
        "## Part 1 - Buliding The CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHCE5Ri1GQWg"
      },
      "source": [
        "## Importing the Keras Libraries and Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEIdCCV5GQWi"
      },
      "source": [
        "import tensorflow\n",
        "import keras\n",
        "from keras.models import Sequential # We will use this to initialize our Neural Netowrk\n",
        "# Since we are using images we use 2D, unlike videos which are 3D with time\n",
        "from keras.layers import Convolution2D # Convolution 2D is the package that wil help us in making the first step of the CNN i.e the Convolution Step in which we add the COnvolutional Layers\n",
        "from keras.layers import MaxPooling2D # This package will help us to proceed to Step 2, ThePooling Step, that will add our Pooling layers\n",
        "from keras.layers import Flatten # This is the package that we will use in Step 3, Flattening in which we convert all the Pooled Feature maps build from the previous 2 steps, into a large feature vector that then becomes the input layer of our fully connected layers \n",
        "from keras.layers import Dense # This is the package we will used to add the fully connected layers in the CNN, like in ANN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYC5npFyGQWk"
      },
      "source": [
        "## Initalising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulPUZYx-GQWk"
      },
      "source": [
        "classifier = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP4i2SCAGQWl"
      },
      "source": [
        "## Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5yTwNNqGQWm"
      },
      "source": [
        "# We create many feature maps to create the Convolutional Layer\n",
        "classifier.add(Convolution2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
        "# filters, rows, columns - The number of filters, the number of feature detectors that we apply to get the same number of feature maps. Along with their dimmensions\n",
        "# 32,3,3 means that we are going to use 32 feature detectors having a 3X3 dimmension, where each feature detector will go through the Input image to create a feature map.\n",
        "# Convolutional layer will comprise of Total of 32 Feature Maps\n",
        "# We will use 32 as it is generally taken for CNN\n",
        "# border_mode = 'same' default value\n",
        "# input_shape - Shape of our Input Image on which we will apply the Feature Detectors through the COnvolution operators to get the various Feature Maps\n",
        "# B/WImage - 1 channel, 2D array, RGB(Colored) - 3 channels, 3D array (Each channel corresponds to 1 color, Red, Green or Blue)\n",
        "# And each channel corresponds to one 2D array that contains the pixels of our images\n",
        "# 3 - RGB, 2 - B/W\n",
        "# 64 X 64 - Dimmension of the 2D array (pixels)\n",
        "# Since we are Tensorflow backend - input_shape = (64,64,3)\n",
        "# Theano Backend - input_shape = (3,64,64)\n",
        "# Activation - ReLU as we want to increase Non-Linearity (Rectifier Linear Unit)(Rectifier Activation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyrpVYnVGQWm"
      },
      "source": [
        "## Step 2 - Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Gt73iOGQWn"
      },
      "source": [
        "# We take the maximum with a stride of 2, Max Pooling\n",
        "# For odd dimmension of Feature map - Dimmension of Pooled Feature map : n+1 X n+1, For even dimmension of Feature map - Dimmension of Pooled Feature map : n X n\n",
        "# We apply Max Pooling on each and every Feature Map\n",
        "# We apply Max Pooling to reduce the size, spatial invarince, avoids overfitting and we are keeping the important features, reduces time complexity as well\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "# pool_size = (2,2) is the optimal dimmension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqoxYNO5GQWo"
      },
      "source": [
        "## Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcTCkWFYGQWp"
      },
      "source": [
        "# Input layer of Future ANN\n",
        "# By creating the Feature Maps we extracted the spatial structure informations, by getting high numbers in the Feature Maps, because of the Feature Detectors\n",
        "# The high numbers in the feature maps are associated to a specific feature in the input image\n",
        "# And after applying Max Pooling we still keep all these high numbers, because we take the max.\n",
        "# Flattening step consists of putting all the numbers in the cells of the feature maps into a single vector\n",
        "# The spatial structure information gets stored in the huge single 1D vector\n",
        "# If we directly flatten the input image pixels into the huge single 1D vector, then each node of this huge vector will represent one pixel of the image independent of the pixels around it.\n",
        "# So we only get informations about the pixel itself and not about how the pixel is spatially connected to the other pixels around it. Thus we don't get anyinformation about the spatial structure around this pixel.\n",
        "# That is why we need to follow all the previous steps\n",
        "classifier.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVi-tox0GQWp"
      },
      "source": [
        "## Step 4 - Fully Connected Layers (ANN) Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CuOKcqGGQWq"
      },
      "source": [
        "classifier.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "# We have a lot of input nodes, we should'nt choose a number that is too small or too big. units is seclected by trying on diff values\n",
        "# units = Common practice is to pick a power of 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWyVBif2GQWq"
      },
      "source": [
        "## Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ30k4xeGQWq"
      },
      "source": [
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "# loss = It corresponds to the logarithmic class of Cost Function(Logistic), 'binary_crossentropy' for binary outcomes, Here cat or dog\n",
        "# if we have more than two like cat, dog, and birds etc, then we will choose categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a0VkI5oGQWr"
      },
      "source": [
        "## Part 2 - Fitting the CNN to out Images\n",
        "## Image Preprocessing Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoLVDLwXGQWr"
      },
      "source": [
        "#### What is Image Augumentation? and How does it prevent Overfitting?\n",
        "\n",
        "We know that one of the situations that lead to overfitting is when we have few data to train our model, in that situation our model can find some correlatios in the few observations of the training set, but fails to generalize these correlations on some new observations, And when it comes to images we actually need a lot of images to generalize some correlations, Because in Computer Vision, the ML model needs to find some patterns in the pixels and to do this it requires a lot of images.\n",
        "\n",
        "If we don't do Image Augumentation, what we might get is a result with great accuracy on the training set but a much lower accuracy on the test set\n",
        "\n",
        "Right now, we are working with 10,000 images, 8000 for training set and 2000 for test set, and that is actually not much to obtain a great performance and that is where we will use Image Augumentation.\n",
        "\n",
        "\n",
        "It basicallly creates many batches of our images, and then on each batch it applies some transformations on a random selection of our image, like rotating them, flipping them, snipping them or even shearing them. Thus we will get a lot of diverse images inside these batches and therefore a lot more material to train. And this is why it is called Image Augumentation.\n",
        "The model will never find same picture across the batches as it is random.\n",
        "\n",
        "### It is a technique that allows to enrich our datset, our training set without adding more images thus allowing us to get good performance results with little or no overfitting even with a small amount of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50gBv5xhGQWr",
        "outputId": "60323d9a-6a60-4f54-a5c7-3222daa0798b"
      },
      "source": [
        "# We will take help from the Keras Documentation\n",
        "# First function we will use is to generate Image Augumentation: ImageDataGenerator,\n",
        "# There are two ways: .flow(x, y):, .flow_from_directory(directory):\n",
        "# We will use .flow_from_directory(directory): # As we have a directory to import the dateset from.\n",
        "from keras.preprocessing.image import ImageDataGenerarto\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,   # feature scaling(0 and 1)  ############\n",
        "        shear_range=0.2,  # Shearing (also called transvection)  # Applying several transformations, {Image Augumentation}\n",
        "        zoom_range=0.2,   # random zoom                          # on Training Set\n",
        "        horizontal_flip=True)    # Flip               ############\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)   # Only rescaling on Test Set\n",
        "\n",
        "# Creates the training set and test set\n",
        "training_set = train_datagen.flow_from_directory(r'D:\\dataset\\training_set',\n",
        "                                                 target_size=(64, 64),  # Size of images expected in CNN Model\n",
        "                                                 batch_size=1,     # Size of our bathces, after which weights will get updated\n",
        "                                                 class_mode='binary')  # Types of classes\n",
        "                                                    \n",
        "test_set = test_datagen.flow_from_directory(r'D:\\dataset\\test_set',\n",
        "                                            target_size=(64, 64),  # Size of images expected in CNN Model\n",
        "                                            batch_size=1,     # Size of our bathces, after which weights will get updated\n",
        "                                            class_mode='binary')  # Types of classes\n",
        "        \n",
        "\n",
        "# Fits the CNN model on the training set as well as testing its performance on the test set\n",
        "classifier.fit_generator(training_set,\n",
        "                        steps_per_epoch=8000,  # Number of images we have in our training set (Since we have 8000 training set data)\n",
        "                        epochs=25,   # Number of epochs\n",
        "                        validation_data=test_set,\n",
        "                        validation_steps=2000)  # Number of images we have in our test set (Since we have 2000 training set data)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From <ipython-input-8-e1b9d2bcd311>:28: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/25\n",
            "8000/8000 [==============================] - 404s 51ms/step - loss: 0.6865 - accuracy: 0.5573 - val_loss: 0.6738 - val_accuracy: 0.5880\n",
            "Epoch 2/25\n",
            "8000/8000 [==============================] - 337s 42ms/step - loss: 0.6401 - accuracy: 0.6426 - val_loss: 0.6423 - val_accuracy: 0.6490\n",
            "Epoch 3/25\n",
            "8000/8000 [==============================] - 261s 33ms/step - loss: 0.6167 - accuracy: 0.6678 - val_loss: 0.5968 - val_accuracy: 0.6890\n",
            "Epoch 4/25\n",
            "8000/8000 [==============================] - 260s 32ms/step - loss: 0.6010 - accuracy: 0.6764 - val_loss: 0.5898 - val_accuracy: 0.6940\n",
            "Epoch 5/25\n",
            "8000/8000 [==============================] - 281s 35ms/step - loss: 0.5886 - accuracy: 0.6927 - val_loss: 0.6403 - val_accuracy: 0.6770\n",
            "Epoch 6/25\n",
            "8000/8000 [==============================] - 288s 36ms/step - loss: 0.5721 - accuracy: 0.7059 - val_loss: 0.6059 - val_accuracy: 0.6800\n",
            "Epoch 7/25\n",
            "8000/8000 [==============================] - 257s 32ms/step - loss: 0.5648 - accuracy: 0.7136 - val_loss: 0.5830 - val_accuracy: 0.6885\n",
            "Epoch 8/25\n",
            " 296/8000 [>.............................] - ETA: 4:20 - loss: 0.5268 - accuracy: 0.7331"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-e1b9d2bcd311>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Fits the CNN model on the training set as well as testing its performance on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m classifier.fit_generator(training_set,\n\u001b[0m\u001b[0;32m     29\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Number of images we have in our training set (Since we have 8000 training set data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;31m# Number of epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \"\"\"\n\u001b[0;32m   1814\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fit_generator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1816\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSu9iRZ2GQWt"
      },
      "source": [
        "## With 1 Convolutional layer we got a Training set accuracy of 81.42% and a Test set accuracy of 71.60%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNQDhBlGQWu"
      },
      "source": [
        "## In order to increase our accuracy:\n",
        "1. Add another Convolutional Layer (Best) {After Max Pooling Step}\n",
        "2. Add another Fully Connected Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rvz5ryhGQWu"
      },
      "source": [
        "#### Adding a 2nd Convolutional Layer to improve the accuracy\n",
        "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))  # No need for input_shape as keras knows\n",
        "\n",
        "classifier.add(MaxPooling2D(pool_size = (2,2))) # pool_size = (2,2) is the optimal dimmension\n",
        "\n",
        "If we add another Convolutional Layer we can double the number of feature detectors, instead of 32 we can talk 64."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3hPkex9GQWu"
      },
      "source": [
        "## Part 3 - Predicting new Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYIONgzsGQWv"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('D:\\ML Python\\P14-Part8-Deep-Learning\\Section 36 - Convolutional Neural Networks\\dataset\\single_prediction\\cat_or_dog_2.jpg', target_size=(64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)   # Used to add the 4th dimmension, This new dimmension corresponds to the batch\n",
        "result = classifier.predict(test_image)  # We need 4 Dimenssions insted of 3D\n",
        "# In general the functions of neural networks cannot accept a single input by itself, like in this case for a single image,\n",
        "# It always accepts inputs in a batch, even if batch = 1 or several inputs\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'Dog'\n",
        "else:\n",
        "    prediction = 'Cat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to2rMLC9GQWv",
        "outputId": "081f8a11-707f-40e0-81b6-e165633094b8"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1570709]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVHO3X77GQWw",
        "outputId": "177a92db-1b57-4b25-f9d1-a9ddc6449060"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKPb-PKFGQWw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}